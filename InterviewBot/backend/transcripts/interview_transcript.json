[
    {
        "type": "JD",
        "question": "How would you approach optimizing a generative model for deployment in a resource-constrained environment?\n[Follow-up Question]: Can you describe any general strategies for reducing the computational cost of machine learning models, even if you're not familiar with generative models specifically?\n[Follow-up Question]: Can you describe any techniques you've used to reduce the size or improve the speed of a machine learning model, even if it was a different type of model than a generative model?\n[Follow-up Question]: Can you elaborate on how limiting context size impacted the model's performance and resource usage in your experience?\n[Follow-up Question]: Can you give a specific example of a task where limiting context size significantly degraded the model's performance, and quantify the impact if possible?",
        "answer": "i dont know how\n[Follow-up Answer]: im not sure but i did use GEnai\n[Follow-up Answer]: yes i used paraments like limiting context size limiting as such\n[Follow-up Answer]: Limiting context size in a large language model (LLM) primarily leads to faster performance and lower resource usage, but significantly degrades the model's ability to handle complex tasks requiring a broad understanding of information\n[Follow-up Answer]: The Task: Multi-Document Question Answering\nImagine a task where a user asks a specific question that requires synthesizing information spread across a large volume of text (e.g., a 100-page report), such as: \"What were the specific reasons the company cited for the Q3 revenue drop, and how do those compare to the legal risks highlighted in the compliance section?\""
    }
]