{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84add766",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joshua.david\\Downloads\\InterviewBot\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created 36 chunks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'resume', 'chunk_id': 0}, page_content='John Doe\\nSan Francisco, CA | johndoe@email.com | (123) 456-7890 | linkedin.com/in/johndoe | github.com/johndoe'),\n",
       " Document(metadata={'source': 'resume', 'chunk_id': 1}, page_content='Generative AI Engineer'),\n",
       " Document(metadata={'source': 'resume', 'chunk_id': 2}, page_content='Innovative and detail-oriented Generative AI Engineer with 1.5 years of experience designing, training, and deploying large language models (LLMs) and multimodal AI systems'),\n",
       " Document(metadata={'source': 'resume', 'chunk_id': 3}, page_content='. Skilled in prompt engineering, fine-tuning transformer architectures, and building scalable generative pipelines using modern ML frameworks.'),\n",
       " Document(metadata={'source': 'resume', 'chunk_id': 4}, page_content='PROFESSIONAL EXPERIENCE\\n\\nAI Engineer\\nNeuraLabs Inc., San Francisco, CA\\nJune 2023 ‚Äì Present'),\n",
       " Document(metadata={'source': 'resume', 'chunk_id': 5}, page_content='Developed and deployed a fine-tuned Llama-2-based customer support chatbot, reducing human agent workload by 35% and improving response accuracy by 22%.'),\n",
       " Document(metadata={'source': 'resume', 'chunk_id': 6}, page_content='Built a multimodal content generation pipeline using CLIP and Stable Diffusion to auto-generate marketing visuals from text prompts, adopted by 3 internal product teams.\\nOptimized inference latency by 40% through quantization and ONNX runtime integration for generative models in production.'),\n",
       " Document(metadata={'source': 'resume', 'chunk_id': 7}, page_content='Collaborated with product and data teams to implement human-in-the-loop feedback systems for continuous model improvement.\\nPROJECTS'),\n",
       " Document(metadata={'source': 'resume', 'chunk_id': 8}, page_content='1. Personalized Story Generator'),\n",
       " Document(metadata={'source': 'resume', 'chunk_id': 9}, page_content='Created a fine-tuned T5 model that generates short fiction stories tailored to user preferences (genre, tone, characters).\\nIntegrated retrieval-augmented generation (RAG) to maintain narrative consistency and fact grounding.'),\n",
       " Document(metadata={'source': 'resume', 'chunk_id': 10}, page_content='Deployed as a Flask API with React frontend; achieved 4.7/5 user satisfaction in beta testing.'),\n",
       " Document(metadata={'source': 'resume', 'chunk_id': 11}, page_content='2. CodeWhisperer Clone (Open-Source)'),\n",
       " Document(metadata={'source': 'resume', 'chunk_id': 12}, page_content='Trained a 160M-parameter code generation model on GitHub public repositories using CodeLlama architecture.\\nImplemented context-aware autocompletion for Python and JavaScript with 82% syntactic correctness in evaluations.\\nOpen-sourced on GitHub; received 450+ stars and 30+ forks within 2 months.'),\n",
       " Document(metadata={'source': 'resume', 'chunk_id': 13}, page_content='3. AI-Powered Resume Builder'),\n",
       " Document(metadata={'source': 'resume', 'chunk_id': 14}, page_content='Engineered a generative system that transforms user inputs into ATS-optimized resumes using few-shot prompting with GPT-3.5 Turbo.\\nAdded PDF export and keyword optimization features based on real job descriptions.\\nHosted on AWS Lambda with S3 storage; used by 1,200+ beta users.\\n\\nCERTIFICATIONS'),\n",
       " Document(metadata={'source': 'resume', 'chunk_id': 15}, page_content='CERTIFICATIONS\\n\\nAWS Certified Machine Learning ‚Äì Specialty (2024)\\nDeepLearning.AI Generative AI with LLMs Specialization (2023)\\nGoogle Cloud Professional Machine Learning Engineer (2023)\\n\\nTECHNICAL SKILLS'),\n",
       " Document(metadata={'source': 'resume', 'chunk_id': 16}, page_content='Languages: Python, JavaScript, SQL\\nML Frameworks: PyTorch, Hugging Face Transformers, LangChain, LlamaIndex\\nCloud/DevOps: AWS (SageMaker, Lambda, S3), Docker, FastAPI, Git\\nGenerative AI: LLM fine-tuning (LoRA, QLoRA), RAG, Prompt Engineering, Stable Diffusion, Vector Databases (Pinecone, FAISS)'),\n",
       " Document(metadata={'source': 'resume', 'chunk_id': 17}, page_content='EDUCATION\\n\\nB.S. in Computer Science\\nUniversity of California, Berkeley ‚Äì 2022'),\n",
       " Document(metadata={'source': 'job_description', 'chunk_id': 18}, page_content='Generative AI Engineer job brief'),\n",
       " Document(metadata={'source': 'job_description', 'chunk_id': 19}, page_content='We are seeking a skilled Generative AI Engineer to join our team and lead the development of innovative AI solutions. Your expertise in generative models, deep learning, and data analysis will be critical in creating intelligent and transformative AI applications'),\n",
       " Document(metadata={'source': 'job_description', 'chunk_id': 20}, page_content='. You will work closely with cross-functional teams to conceptualize, design, test, and deploy AI projects that drive innovation and provide value in the rapidly evolving field of artificial intelligence. Join us and be part of a dynamic team that is shaping the future of AI.'),\n",
       " Document(metadata={'source': 'job_description', 'chunk_id': 21}, page_content='Generative AI Engineer job responsibilities:\\n- Design and develop algorithms for generative models using deep learning techniques.\\n\\n- Collaborate with cross-functional teams to integrate generative AI solutions into existing workflow systems.'),\n",
       " Document(metadata={'source': 'job_description', 'chunk_id': 22}, page_content='- Research and stay up-to-date on the latest advancements in generative AI technologies and methodologies.\\n\\n- Optimize and fine-tune generative models for performance and efficiency.\\n\\n- Troubleshoot and resolve issues related to generative AI models and implementations.'),\n",
       " Document(metadata={'source': 'job_description', 'chunk_id': 23}, page_content='- Create and maintain documentation for generative AI models and their applications.\\n\\n- Communicate complex technical concepts and findings to non-technical stakeholders.\\n\\nGenerative AI Engineer job requirements:\\n- Strong background in machine learning and deep learning algorithms.'),\n",
       " Document(metadata={'source': 'job_description', 'chunk_id': 24}, page_content='- Proficiency in Python, TensorFlow, and PyTorch for developing AI models.\\n\\n- Experience in generative AI techniques such as GANs and VAEs.\\n\\n- Ability to design and implement scalable and efficient AI systems.\\n\\n- Advanced knowledge of natural language processing for text generation tasks.'),\n",
       " Document(metadata={'source': 'job_description', 'chunk_id': 25}, page_content='- Familiarity with computer vision and image generation using AI.\\n\\n- Skills in data preprocessing and feature engineering for AI model training.\\n\\n- Strong understanding of neural network architectures and optimization techniques.\\n\\n- Experience in deploying AI models into production environments.'),\n",
       " Document(metadata={'source': 'job_description', 'chunk_id': 26}, page_content='- Ability to stay updated with the latest advancements in generative AI research and incorporate them into work.'),\n",
       " Document(metadata={'source': 'job_description', 'chunk_id': 27}, page_content='Join our team as a Generative AI Engineer\\n- Joining our team as a Generative AI Engineer means being at the forefront of innovation, working on cutting-edge projects that are shaping the future of AI and machine learning.'),\n",
       " Document(metadata={'source': 'job_description', 'chunk_id': 28}, page_content=\"- As a Generative AI Engineer on our team, you'll have the opportunity to collaborate with top experts in the field, contributing to groundbreaking research and development that has real-world impact.\"),\n",
       " Document(metadata={'source': 'job_description', 'chunk_id': 29}, page_content=\"- We offer a dynamic and collaborative work environment where your ideas and contributions are valued, and where you'll have the resources and support needed to bring your vision to life.\"),\n",
       " Document(metadata={'source': 'job_description', 'chunk_id': 30}, page_content='- Joining our team as a Generative AI Engineer means being part of a culture that fosters continuous learning and professional growth, with access to ongoing training and development opportunities.'),\n",
       " Document(metadata={'source': 'job_description', 'chunk_id': 31}, page_content=\"- As a Generative AI Engineer with our team, you'll have the chance to work on diverse and challenging projects, gaining valuable experience and expertise that will set you apart in your career.\"),\n",
       " Document(metadata={'source': 'job_description', 'chunk_id': 32}, page_content='Generative AI Engineer salary and benefits:'),\n",
       " Document(metadata={'source': 'job_description', 'chunk_id': 33}, page_content=\"In today's competitive job market, transparency is key to attracting top talent\"),\n",
       " Document(metadata={'source': 'job_description', 'chunk_id': 34}, page_content='. 50000$ salary range and benefits in your Generative AI Engineer job description, Offering a competitive salary range, along with benefits like remote work options, ongoing education stipends, or comprehensive health care packages,'),\n",
       " Document(metadata={'source': 'job_description', 'chunk_id': 35}, page_content=\"also distinguish our organization in a field rife with opportunities. Ultimately, transparently presenting the full compensation package is a smart, strategic move that attracts candidates who are not just highly skilled but also aligned with our company's values and long-term vision.\")]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# === 1. Load documents ===\n",
    "resume_loader = TextLoader(\"resume_candidate_a.txt\", encoding=\"utf-8\")\n",
    "job_loader = TextLoader(\"jd_genai_engineer.txt\", encoding=\"utf-8\")\n",
    "\n",
    "resume_docs = resume_loader.load()\n",
    "job_docs = job_loader.load()\n",
    "\n",
    "# Add metadata to distinguish sources\n",
    "for doc in resume_docs:\n",
    "    doc.metadata[\"source\"] = \"resume\"\n",
    "for doc in job_docs:\n",
    "    doc.metadata[\"source\"] = \"job_description\"\n",
    "\n",
    "all_docs = resume_docs + job_docs\n",
    "\n",
    "# === 2. Semantic Chunking (Agentic-style) ===\n",
    "# Use smaller chunk_size to preserve job/resume sections\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=50,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
    "    keep_separator=True\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(all_docs)\n",
    "\n",
    "# Optional: Add chunk index for debugging\n",
    "for i, chunk in enumerate(chunks):\n",
    "    chunk.metadata[\"chunk_id\"] = i\n",
    "\n",
    "print(f\"‚úÖ Created {len(chunks)} chunks\")\n",
    "\n",
    "# === 3. Embed & Store in FAISS ===\n",
    "embeddings = HuggingFaceEmbeddings(model=\"hkunlp/instructor-large\")\n",
    "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "# Save for later (optional)\n",
    "# vectorstore.save_local(\"faiss_resume_job_index\")\n",
    "\n",
    "# === 4. Test Retrieval ===\n",
    "chunks\n",
    "# query = \"What Python and AWS experience does the candidate have, and does it match the job requirements?\"\n",
    "\n",
    "# # Get top 5 most relevant chunks with scores\n",
    "# retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "# docs_and_scores = vectorstore.similarity_search_with_relevance_scores(query, k=5)\n",
    "\n",
    "# print(\"\\nüîç Retrieval Results:\\n\")\n",
    "# for i, (doc, score) in enumerate(docs_and_scores):\n",
    "#     print(f\"--- Chunk {i+1} (Score: {score:.4f}) ---\")\n",
    "#     print(f\"Source: {doc.metadata['source']}\")\n",
    "#     print(f\"Content: {doc.page_content[:300]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d615ea06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Retrieval Results:\n",
      "\n",
      "--- Chunk 1 (Score: 0.8516) ---\n",
      "Source: job_description\n",
      "Content: - Proficiency in Python, TensorFlow, and PyTorch for developing AI models.\n",
      "\n",
      "- Experience in generative AI techniques such as GANs and VAEs.\n",
      "\n",
      "- Ability to design and implement scalable and efficient AI systems.\n",
      "\n",
      "- Advanced knowledge of natural language processing for text generation tasks....\n",
      "\n",
      "--- Chunk 2 (Score: 0.8308) ---\n",
      "Source: job_description\n",
      "Content: - Familiarity with computer vision and image generation using AI.\n",
      "\n",
      "- Skills in data preprocessing and feature engineering for AI model training.\n",
      "\n",
      "- Strong understanding of neural network architectures and optimization techniques.\n",
      "\n",
      "- Experience in deploying AI models into production environments....\n",
      "\n",
      "--- Chunk 3 (Score: 0.8298) ---\n",
      "Source: resume\n",
      "Content: CERTIFICATIONS\n",
      "\n",
      "AWS Certified Machine Learning ‚Äì Specialty (2024)\n",
      "DeepLearning.AI Generative AI with LLMs Specialization (2023)\n",
      "Google Cloud Professional Machine Learning Engineer (2023)\n",
      "\n",
      "TECHNICAL SKILLS...\n",
      "\n",
      "--- Chunk 4 (Score: 0.8187) ---\n",
      "Source: resume\n",
      "Content: Innovative and detail-oriented Generative AI Engineer with 1.5 years of experience designing, training, and deploying large language models (LLMs) and multimodal AI systems...\n",
      "\n",
      "--- Chunk 5 (Score: 0.8045) ---\n",
      "Source: job_description\n",
      "Content: Generative AI Engineer job brief...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What Python and AWS experience does the candidate have, and does it match the job requirements?\"\n",
    "\n",
    "# Get top 5 most relevant chunks with scores\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "docs_and_scores = vectorstore.similarity_search_with_relevance_scores(query, k=5)\n",
    "\n",
    "print(\"\\nüîç Retrieval Results:\\n\")\n",
    "for i, (doc, score) in enumerate(docs_and_scores):\n",
    "    print(f\"--- Chunk {i+1} (Score: {score:.4f}) ---\")\n",
    "    print(f\"Source: {doc.metadata['source']}\")\n",
    "    print(f\"Content: {doc.page_content[:300]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f44ffb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Agentic Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a5a500d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyC96fwdtFTkgtQaYA0wtbsktG7PV_VOa8M\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "509d6100",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_loader = TextLoader(\"resume_candidate_a.txt\", encoding=\"utf-8\")\n",
    "job_loader = TextLoader(\"jd_genai_engineer.txt\", encoding=\"utf-8\")\n",
    "\n",
    "resume_text = resume_loader.load()\n",
    "job_text = job_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2caeb524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Agent identified these requirements to verify:\n",
      "1. Here's a list of specific, verifiable requirements from the job description:\n",
      "2. *   Background in machine learning algorithms\n",
      "3. *   Background in deep learning algorithms\n",
      "4. *   Proficiency in Python\n",
      "5. *   Proficiency in TensorFlow\n",
      "6. *   Proficiency in PyTorch\n",
      "7. *   Experience with GANs (Generative Adversarial Networks)\n",
      "8. *   Experience with VAEs (Variational Autoencoders)\n",
      "9. *   Ability to design scalable AI systems\n",
      "10. *   Ability to implement scalable AI systems\n",
      "11. *   Ability to design efficient AI systems\n",
      "12. *   Ability to implement efficient AI systems\n",
      "13. *   Advanced knowledge of NLP for text generation tasks\n",
      "14. *   Familiarity with computer vision\n",
      "15. *   Familiarity with image generation using AI\n",
      "16. *   Skills in data preprocessing\n",
      "17. *   Skills in feature engineering\n",
      "18. *   Understanding of neural network architectures\n",
      "19. *   Understanding of neural network optimization techniques\n",
      "20. *   Experience deploying AI models to production environments\n",
      "21. *   Ability to stay updated with generative AI research\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 24.322744475s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 24\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 21.946474483s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 21\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 8.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 17.586904156s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 17\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 16.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
      "Please retry in 9.236449481s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 9\n",
      "}\n",
      "].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Created 36 AGENTIC chunks (evidence + requirements)\n",
      "\n",
      "üéØ Retrieved Agentic Chunks:\n",
      "\n",
      "--- Match 1 (Score: 0.861) ---\n",
      "Source: job_description\n",
      "Requirement: N/A\n",
      "Content: Here's a list of specific, verifiable requirements from the job description:\n",
      "\n",
      "--- Match 2 (Score: 0.842) ---\n",
      "Source: job_description\n",
      "Requirement: N/A\n",
      "Content: *   Ability to implement efficient AI systems\n",
      "\n",
      "--- Match 3 (Score: 0.841) ---\n",
      "Source: job_description\n",
      "Requirement: N/A\n",
      "Content: *   Ability to implement scalable AI systems\n",
      "\n",
      "--- Match 4 (Score: 0.840) ---\n",
      "Source: job_description\n",
      "Requirement: N/A\n",
      "Content: *   Familiarity with computer vision\n",
      "\n",
      "--- Match 5 (Score: 0.835) ---\n",
      "Source: job_description\n",
      "Requirement: N/A\n",
      "Content: *   Proficiency in PyTorch\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# === 1. Load JD (primary) and Resume (secondary) ===\n",
    "jd_text = TextLoader(\"txts/job_description.txt\", encoding=\"utf-8\").load()[0].page_content\n",
    "resume_text = TextLoader(\"txts/resume.txt\", encoding=\"utf-8\").load()[0].page_content\n",
    "\n",
    "# === 2. STEP 1: LLM reads JD ‚Üí generates verification questions ===\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0.3)\n",
    "\n",
    "question_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a senior hiring agent. Based on the job description, generate a list of 3-5 specific, factual questions to verify a candidate's qualifications. \"\n",
    "                \"Each question must be answerable by looking at a resume. Focus on skills, experience duration, tools, or domains mentioned in the JD.\"),\n",
    "    (\"human\", \"Job Description:\\n{jd}\\n\\nGenerate verification questions (one per line):\")\n",
    "])\n",
    "\n",
    "questions_chain = question_prompt | llm\n",
    "response = questions_chain.invoke({\"jd\": jd_text})\n",
    "questions = [q.strip(\"-‚Ä¢ 1234567890. \") for q in response.content.strip().split(\"\\n\") if q.strip()]\n",
    "\n",
    "print(\"üìã JD-Driven Verification Questions:\")\n",
    "for i, q in enumerate(questions, 1):\n",
    "    print(f\"{i}. {q}\")\n",
    "\n",
    "# === 3. STEP 2: Prepare Resume for Retrieval (Chunk or Keep Whole) ===\n",
    "# For resumes, often better to keep as 1-2 chunks (they're short)\n",
    "# But we'll split by section if possible\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "resume_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=400,\n",
    "    chunk_overlap=50,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \"]\n",
    ")\n",
    "resume_chunks = resume_splitter.create_documents([resume_text])\n",
    "# Mark all as resume source\n",
    "for chunk in resume_chunks:\n",
    "    chunk.metadata[\"source\"] = \"resume\"\n",
    "\n",
    "# Create vector store from RESUME ONLY\n",
    "embeddings = HuggingFaceEmbeddings(model=\"text-embedding-3-small\")\n",
    "resume_vectorstore = FAISS.from_documents(resume_chunks, embeddings)\n",
    "resume_retriever = resume_vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "# === 4. STEP 3: For each JD-driven question, retrieve FROM RESUME ONLY ===\n",
    "print(\"\\nüîç Retrieving Evidence from Resume:\\n\")\n",
    "\n",
    "all_results = []\n",
    "for q in questions:\n",
    "    print(f\"‚ùì Question: {q}\")\n",
    "    # Retrieve ONLY from resume\n",
    "    retrieved = resume_vectorstore.similarity_search_with_relevance_scores(q, k=2)\n",
    "    \n",
    "    evidence = []\n",
    "    for doc, score in retrieved:\n",
    "        if score > 0.4:  # Only show relevant matches\n",
    "            print(f\"  ‚Üí (Score: {score:.3f}) {doc.page_content[:200]}...\")\n",
    "            evidence.append(doc.page_content)\n",
    "        else:\n",
    "            print(\"  ‚Üí No strong evidence found.\")\n",
    "    \n",
    "    all_results.append({\n",
    "        \"question\": q,\n",
    "        \"evidence\": evidence,\n",
    "        \"retrieved_chunks\": retrieved\n",
    "    })\n",
    "    print()\n",
    "\n",
    "# === OPTIONAL: Save for deeper analysis ===\n",
    "# You can now feed `all_results` to an LLM for final scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83d6229b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'resume', 'requirement': '*   Background in machine learning algorithms', 'chunk_type': 'agentic_evidence'}, page_content='Skilled in prompt engineering, fine-tuning transformer architectures, and building scalable generative pipelines using modern ML frameworks. AWS Certified Machine Learning ‚Äì Specialty (2024). Google Cloud Professional Machine Learning Engineer (2023).'),\n",
       " Document(metadata={'source': 'resume', 'requirement': '*   Background in deep learning algorithms', 'chunk_type': 'agentic_evidence'}, page_content='Generative AI Engineer with 1.5 years of experience designing, training, and deploying large language models (LLMs) and multimodal AI systems. Skilled in fine-tuning transformer architectures. DeepLearning.AI Generative AI with LLMs Specialization (2023).'),\n",
       " Document(metadata={'source': 'resume', 'requirement': '*   Proficiency in Python', 'chunk_type': 'agentic_evidence'}, page_content='Languages: Python, JavaScript, SQL\\nImplemented context-aware autocompletion for Python and JavaScript with 82% syntactic correctness in evaluations.'),\n",
       " Document(metadata={'source': 'resume', 'requirement': '*   Proficiency in PyTorch', 'chunk_type': 'agentic_evidence'}, page_content='ML Frameworks: PyTorch, Hugging Face Transformers, LangChain, LlamaIndex'),\n",
       " Document(metadata={'source': 'resume', 'requirement': '*   Ability to design scalable AI systems', 'chunk_type': 'agentic_evidence'}, page_content='Skilled in prompt engineering, fine-tuning transformer architectures, and building scalable generative pipelines using modern ML frameworks.'),\n",
       " Document(metadata={'source': 'resume', 'requirement': '*   Ability to implement scalable AI systems', 'chunk_type': 'agentic_evidence'}, page_content='Skilled in prompt engineering, fine-tuning transformer architectures, and building scalable generative pipelines using modern ML frameworks. Optimized inference latency by 40% through quantization and ONNX runtime integration for generative models in production. Hosted on AWS Lambda with S3 storage; used by 1,200+ beta users.'),\n",
       " Document(metadata={'source': 'resume', 'requirement': '*   Ability to design efficient AI systems', 'chunk_type': 'agentic_evidence'}, page_content='Innovative and detail-oriented Generative AI Engineer with 1.5 years of experience designing, training, and deploying large language models (LLMs) and multimodal AI systems. Optimized inference latency by 40% through quantization and ONNX runtime integration for generative models in production.'),\n",
       " Document(metadata={'source': 'resume', 'requirement': '*   Ability to implement efficient AI systems', 'chunk_type': 'agentic_evidence'}, page_content='Optimized inference latency by 40% through quantization and ONNX runtime integration for generative models in production.'),\n",
       " Document(metadata={'source': 'resume', 'requirement': '*   Advanced knowledge of NLP for text generation tasks', 'chunk_type': 'agentic_evidence'}, page_content='Innovative and detail-oriented Generative AI Engineer with 1.5 years of experience designing, training, and deploying large language models (LLMs). Skilled in prompt engineering, fine-tuning transformer architectures. Created a fine-tuned T5 model that generates short fiction stories tailored to user preferences. Integrated retrieval-augmented generation (RAG) to maintain narrative consistency and fact grounding. Engineered a generative system that transforms user inputs into ATS-optimized resumes using few-shot prompting with GPT-3.5 Turbo. Generative AI: LLM fine-tuning (LoRA, QLoRA), RAG, Prompt Engineering.'),\n",
       " Document(metadata={'source': 'resume', 'requirement': '*   Familiarity with computer vision', 'chunk_type': 'agentic_evidence'}, page_content='Built a multimodal content generation pipeline using CLIP and Stable Diffusion to auto-generate marketing visuals from text prompts, adopted by 3 internal product teams.'),\n",
       " Document(metadata={'source': 'resume', 'requirement': '*   Familiarity with image generation using AI', 'chunk_type': 'agentic_evidence'}, page_content='Built a multimodal content generation pipeline using CLIP and Stable Diffusion to auto-generate marketing visuals from text prompts, adopted by 3 internal product teams.'),\n",
       " Document(metadata={'source': 'resume', 'requirement': '*   Understanding of neural network architectures', 'chunk_type': 'agentic_evidence'}, page_content='Skilled in prompt engineering, fine-tuning transformer architectures, and building scalable generative pipelines using modern ML frameworks.\\nDeveloped and deployed a fine-tuned Llama-2-based customer support chatbot.\\nBuilt a multimodal content generation pipeline using CLIP and Stable Diffusion.\\nCreated a fine-tuned T5 model that generates short fiction stories tailored to user preferences.\\nTrained a 160M-parameter code generation model on GitHub public repositories using CodeLlama architecture.'),\n",
       " Document(metadata={'source': 'resume', 'requirement': '*   Understanding of neural network optimization techniques', 'chunk_type': 'agentic_evidence'}, page_content='Optimized inference latency by 40% through quantization and ONNX runtime integration for generative models in production. Skilled in fine-tuning transformer architectures. LLM fine-tuning (LoRA, QLoRA).'),\n",
       " Document(metadata={'source': 'resume', 'requirement': '*   Experience deploying AI models to production environments', 'chunk_type': 'agentic_evidence'}, page_content='*   \"Innovative and detail-oriented Generative AI Engineer with 1.5 years of experience designing, training, and deploying large language models (LLMs) and multimodal AI systems.\"\\n*   \"Developed and deployed a fine-tuned Llama-2-based customer support chatbot...\"\\n*   \"Optimized inference latency by 40% through quantization and ONNX runtime integration for generative models in production.\"\\n*   \"Deployed as a Flask API with React frontend...\"\\n*   \"Hosted on AWS Lambda with S3 storage...\"'),\n",
       " Document(metadata={'source': 'resume', 'requirement': '*   Ability to stay updated with generative AI research', 'chunk_type': 'agentic_evidence'}, page_content='DeepLearning.AI Generative AI with LLMs Specialization (2023)'),\n",
       " Document(metadata={'source': 'job_description', 'chunk_type': 'requirement'}, page_content=\"Here's a list of specific, verifiable requirements from the job description:\"),\n",
       " Document(metadata={'source': 'job_description', 'chunk_type': 'requirement'}, page_content='*   Background in machine learning algorithms'),\n",
       " Document(metadata={'source': 'job_description', 'chunk_type': 'requirement'}, page_content='*   Background in deep learning algorithms'),\n",
       " Document(metadata={'source': 'job_description', 'chunk_type': 'requirement'}, page_content='*   Proficiency in Python'),\n",
       " Document(metadata={'source': 'job_description', 'chunk_type': 'requirement'}, page_content='*   Proficiency in TensorFlow'),\n",
       " Document(metadata={'source': 'job_description', 'chunk_type': 'requirement'}, page_content='*   Proficiency in PyTorch'),\n",
       " Document(metadata={'source': 'job_description', 'chunk_type': 'requirement'}, page_content='*   Experience with GANs (Generative Adversarial Networks)'),\n",
       " Document(metadata={'source': 'job_description', 'chunk_type': 'requirement'}, page_content='*   Experience with VAEs (Variational Autoencoders)'),\n",
       " Document(metadata={'source': 'job_description', 'chunk_type': 'requirement'}, page_content='*   Ability to design scalable AI systems'),\n",
       " Document(metadata={'source': 'job_description', 'chunk_type': 'requirement'}, page_content='*   Ability to implement scalable AI systems'),\n",
       " Document(metadata={'source': 'job_description', 'chunk_type': 'requirement'}, page_content='*   Ability to design efficient AI systems'),\n",
       " Document(metadata={'source': 'job_description', 'chunk_type': 'requirement'}, page_content='*   Ability to implement efficient AI systems'),\n",
       " Document(metadata={'source': 'job_description', 'chunk_type': 'requirement'}, page_content='*   Advanced knowledge of NLP for text generation tasks'),\n",
       " Document(metadata={'source': 'job_description', 'chunk_type': 'requirement'}, page_content='*   Familiarity with computer vision'),\n",
       " Document(metadata={'source': 'job_description', 'chunk_type': 'requirement'}, page_content='*   Familiarity with image generation using AI'),\n",
       " Document(metadata={'source': 'job_description', 'chunk_type': 'requirement'}, page_content='*   Skills in data preprocessing'),\n",
       " Document(metadata={'source': 'job_description', 'chunk_type': 'requirement'}, page_content='*   Skills in feature engineering'),\n",
       " Document(metadata={'source': 'job_description', 'chunk_type': 'requirement'}, page_content='*   Understanding of neural network architectures'),\n",
       " Document(metadata={'source': 'job_description', 'chunk_type': 'requirement'}, page_content='*   Understanding of neural network optimization techniques'),\n",
       " Document(metadata={'source': 'job_description', 'chunk_type': 'requirement'}, page_content='*   Experience deploying AI models to production environments'),\n",
       " Document(metadata={'source': 'job_description', 'chunk_type': 'requirement'}, page_content='*   Ability to stay updated with generative AI research')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agentic_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b0860a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ Retrieved Agentic Chunks:\n",
      "\n",
      "--- Match 1 (Score: 0.859) ---\n",
      "Source: job_description\n",
      "Requirement: N/A\n",
      "Content: *   Familiarity with computer vision\n",
      "\n",
      "--- Match 2 (Score: 0.825) ---\n",
      "Source: job_description\n",
      "Requirement: N/A\n",
      "Content: *   Skills in data preprocessing\n",
      "\n",
      "--- Match 3 (Score: 0.822) ---\n",
      "Source: job_description\n",
      "Requirement: N/A\n",
      "Content: *   Proficiency in Python\n",
      "\n",
      "--- Match 4 (Score: 0.818) ---\n",
      "Source: job_description\n",
      "Requirement: N/A\n",
      "Content: Here's a list of specific, verifiable requirements from the job description:\n",
      "\n",
      "--- Match 5 (Score: 0.814) ---\n",
      "Source: job_description\n",
      "Requirement: N/A\n",
      "Content: *   Familiarity with image generation using AI\n"
     ]
    }
   ],
   "source": [
    "query = \"Does the candidate have Familiarity with CV?\"\n",
    "retrieved = vectorstore.similarity_search_with_relevance_scores(query, k=5)\n",
    "\n",
    "print(\"\\nüéØ Retrieved Agentic Chunks:\")\n",
    "for i, (doc, score) in enumerate(retrieved):\n",
    "    print(f\"\\n--- Match {i+1} (Score: {score:.3f}) ---\")\n",
    "    print(f\"Source: {doc.metadata['source']}\")\n",
    "    print(f\"Requirement: {doc.metadata.get('requirement', 'N/A')}\")\n",
    "    print(f\"Content: {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40af2768",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d26ae04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6383dd9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "652df953",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Key 'parameters' is not supported in schema, ignoring\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Resume Processing Pipeline ---\n",
      "Invoking Gemini for agentic extraction...\n",
      "Extraction complete.\n",
      "Created 7 semantic chunks.\n",
      "\n",
      "--- Sample Chunk ---\n",
      "page_content='Role: Senior Generative AI Engineer at NeuraLabs Inc. (June 2023 - Present). Responsibilities: Spearheaded end-to-end development of an enterprise LLM platform supporting 10+ internal products; fine-tuned Llama-3-70B and Mistral-7B using QLoRA and DPO, achieving 92% human preference alignment on domain-specific QA tasks. Designed and deployed a multimodal RAG architecture combining CLIP, BLIP-2, and FAISS to ground generative responses in proprietary documentation, reducing hallucination rates by 58%. Led a team of 6 engineers to build a model evaluation framework for generative quality, safety, and latency‚Äîadopted org-wide and integrated into CI/CD pipelines. Reduced cloud inference costs by 65% via dynamic batching, model quantization (GGUF), and speculative decoding; saved $1.2M/year in AWS spend. Authored internal technical standards for prompt versioning, eval harnesses, and ethical AI guardrails.' metadata={'category': 'work_experience', 'company': 'NeuraLabs Inc.', 'role': 'Senior Generative AI Engineer'}\n",
      "--------------------\n",
      "\n",
      "Initializing embedding model and FAISS vector store...\n",
      "FAISS index saved locally to 'faiss_resume_index'.\n",
      "--- Pipeline Completed Successfully ---\n",
      "\n",
      "--- Verification: Loading index and performing search ---\n",
      "Query: 'What is his experience with CI/CD pipelines?'\n",
      "--- Search Results ---\n",
      "Content: Role: Senior Generative AI Engineer at NeuraLabs Inc. (June 2023 - Present). Responsibilities: Spearheaded end-to-end development of an enterprise LLM platform supporting 10+ internal products; fine-tuned Llama-3-70B and Mistral-7B using QLoRA and DPO, achieving 92% human preference alignment on domain-specific QA tasks. Designed and deployed a multimodal RAG architecture combining CLIP, BLIP-2, and FAISS to ground generative responses in proprietary documentation, reducing hallucination rates by 58%. Led a team of 6 engineers to build a model evaluation framework for generative quality, safety, and latency‚Äîadopted org-wide and integrated into CI/CD pipelines. Reduced cloud inference costs by 65% via dynamic batching, model quantization (GGUF), and speculative decoding; saved $1.2M/year in AWS spend. Authored internal technical standards for prompt versioning, eval harnesses, and ethical AI guardrails.\n",
      "Metadata: {'category': 'work_experience', 'company': 'NeuraLabs Inc.', 'role': 'Senior Generative AI Engineer'}\n",
      "\n",
      "Content: Degree: B.S. in Computer Science (Honors) from University of California, Berkeley (Graduated: 2017).\n",
      "Metadata: {'category': 'education', 'institution': 'University of California, Berkeley'}\n",
      "\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import List, Dict\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LangChain components\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# --- 1. JSON SCHEMA DEFINITION ---\n",
    "# Define the desired structured output as a JSON Schema dictionary.\n",
    "\n",
    "JSON_RESUME_SCHEMA = {\n",
    "    \"title\": \"Resume\",\n",
    "    \"description\": \"The structured representation of a resume.\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"name\": {\"type\": \"string\", \"description\": \"The full name of the candidate.\"},\n",
    "        \"summary\": {\"type\": \"string\", \"description\": \"A brief summary of the candidate's profile.\"},\n",
    "        \"work_experience\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"role\": {\"type\": \"string\", \"description\": \"The job title or role.\"},\n",
    "                    \"company\": {\"type\": \"string\", \"description\": \"The name of the company.\"},\n",
    "                    \"start_date\": {\"type\": \"string\", \"description\": \"The start date of the employment.\"},\n",
    "                    \"end_date\": {\"type\": \"string\", \"description\": \"The end date of the employment (or 'Present').\"},\n",
    "                    \"responsibilities\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
    "                },\n",
    "                \"required\": [\"role\", \"company\", \"responsibilities\"],\n",
    "            },\n",
    "        },\n",
    "        \"education\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"degree\": {\"type\": \"string\", \"description\": \"The degree obtained.\"},\n",
    "                    \"institution\": {\"type\": \"string\", \"description\": \"The name of the institution.\"},\n",
    "                    \"graduation_date\": {\"type\": \"string\", \"description\": \"The graduation date.\"},\n",
    "                },\n",
    "                \"required\": [\"degree\", \"institution\"],\n",
    "            },\n",
    "        },\n",
    "        \"skills\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
    "    },\n",
    "    \"required\": [\"name\", \"summary\", \"work_experience\", \"education\", \"skills\"],\n",
    "}\n",
    "\n",
    "# --- 2. AGENTIC EXTRACTION (LANGCHAIN + GEMINI) ---\n",
    "\n",
    "def extract_resume_data(resume_text: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Uses LangChain and Gemini to extract structured data from resume text\n",
    "    based on a JSON schema dictionary.\n",
    "    \"\"\"\n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0.3)\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", \"You are an expert HR assistant specializing in parsing resumes. Your task is to extract relevant information\"\n",
    "            \"from the following resume text and format it as a valid JSON object. Adhere strictly to the provided schema.\"\n",
    "            \"If a piece of information is not found, use null or an empty list.\"),\n",
    "            (\"human\", \"{resume_text}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "    # Chain the prompt and model with the structured output schema\n",
    "    structured_runnable = prompt | llm.with_structured_output(schema=JSON_RESUME_SCHEMA)\n",
    "\n",
    "    print(\"Invoking Gemini for agentic extraction...\")\n",
    "    result = structured_runnable.invoke({\"resume_text\": resume_text})\n",
    "    print(\"Extraction complete.\")\n",
    "    return result\n",
    "\n",
    "# --- 3. SEMANTIC CHUNKING ---\n",
    "\n",
    "def create_semantic_chunks(resume_data: Dict) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Converts the structured dictionary into a list of semantic LangChain Documents.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "\n",
    "    # Summary chunk\n",
    "    chunks.append(Document(\n",
    "        page_content=f\"Summary: {resume_data.get('summary', '')}\",\n",
    "        metadata={\"category\": \"summary\", \"name\": resume_data.get('name', '')}\n",
    "    ))\n",
    "\n",
    "    # Work Experience chunks\n",
    "    for job in resume_data.get('work_experience', []):\n",
    "        content = (\n",
    "            f\"Role: {job.get('role')} at {job.get('company')} ({job.get('start_date')} - {job.get('end_date')}). \"\n",
    "            f\"Responsibilities: {' '.join(job.get('responsibilities', []))}\"\n",
    "        )\n",
    "        chunks.append(Document(\n",
    "            page_content=content,\n",
    "            metadata={\"category\": \"work_experience\", \"company\": job.get('company'), \"role\": job.get('role')}\n",
    "        ))\n",
    "\n",
    "    # Education chunks\n",
    "    for edu in resume_data.get('education', []):\n",
    "        content = f\"Degree: {edu.get('degree')} from {edu.get('institution')} (Graduated: {edu.get('graduation_date')}).\"\n",
    "        chunks.append(Document(\n",
    "            page_content=content,\n",
    "            metadata={\"category\": \"education\", \"institution\": edu.get('institution')}\n",
    "        ))\n",
    "\n",
    "    # Skills chunk\n",
    "    skills = resume_data.get('skills', [])\n",
    "    if skills:\n",
    "        chunks.append(Document(\n",
    "            page_content=f\"Skills: {', '.join(skills)}\",\n",
    "            metadata={\"category\": \"skills\"}\n",
    "        ))\n",
    "\n",
    "    print(f\"Created {len(chunks)} semantic chunks.\")\n",
    "    return chunks\n",
    "\n",
    "# --- MAIN EXECUTION ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    load_dotenv()\n",
    "\n",
    "    if not os.getenv(\"GOOGLE_API_KEY\"):\n",
    "        raise ValueError(\"GOOGLE_API_KEY not found in environment variables.\")\n",
    "\n",
    "    # --- Build Phase ---\n",
    "    print(\"--- Starting Resume Processing Pipeline ---\")\n",
    "\n",
    "    with open(\"txts/resume.txt\", \"r\", encoding='utf-8') as f:\n",
    "        resume_text = f.read()\n",
    "\n",
    "    structured_resume = extract_resume_data(resume_text)\n",
    "    documents = create_semantic_chunks(structured_resume)\n",
    "\n",
    "    print(\"\\n--- Sample Chunk ---\")\n",
    "    print(documents[1])\n",
    "    print(\"--------------------\\n\")\n",
    "\n",
    "    # --- 4. VECTORIZATION & STORAGE ---\n",
    "    print(\"Initializing embedding model and FAISS vector store...\")\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "    db = FAISS.from_documents(documents, embeddings)\n",
    "    db.save_local(\"faiss_resume_index\")\n",
    "    print(\"FAISS index saved locally to 'faiss_resume_index'.\")\n",
    "    print(\"--- Pipeline Completed Successfully ---\\n\")\n",
    "\n",
    "    # --- 5. VERIFICATION ---\n",
    "    print(\"--- Verification: Loading index and performing search ---\")\n",
    "    loaded_db = FAISS.load_local(\"faiss_resume_index\", embeddings, allow_dangerous_deserialization=True)\n",
    "    retriever = loaded_db.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "    query = \"What is his experience with CI/CD pipelines?\"\n",
    "    results = retriever.invoke(query)\n",
    "\n",
    "    print(f\"Query: '{query}'\")\n",
    "    print(\"--- Search Results ---\")\n",
    "    for doc in results:\n",
    "        print(f\"Content: {doc.page_content}\")\n",
    "        print(f\"Metadata: {doc.metadata}\\n\")\n",
    "    print(\"----------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bafba387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: ''\n",
      "--- Search Results ---\n",
      "Content: Summary: Visionary AI scientist and engineer with 4+ years of experience at the intersection of large language models (LLMs), multimodal systems, and production-scale generative infrastructure. Combines deep theoretical expertise from PhD research in neural representation learning with hands-on experience building and deploying state-of-the-art generative AI products used by millions. Proven ability to lead cross-functional teams, publish high-impact research, and translate academic innovation into business value.\n",
      "Metadata: {'category': 'summary', 'name': 'John Doe'}\n",
      "\n",
      "Content: Degree: B.S. in Computer Science (Honors) from University of California, Berkeley (Graduated: 2017).\n",
      "Metadata: {'category': 'education', 'institution': 'University of California, Berkeley'}\n",
      "\n",
      "Content: Skills: Python, C++, JavaScript, SQL, Rust, Fine-tuning (LoRA, QLoRA, DPO, RLHF), RAG, Prompt Engineering, LLM Agents, Guardrails, Multimodal Models (LLaVA, Stable Diffusion 3, FLUX), PyTorch, Hugging Face Transformers, LangChain, LlamaIndex, vLLM, LightLLM, ONNX, TensorRT-LLM, AWS (SageMaker, Lambda, EKS, S3), GCP Vertex AI, Docker, Kubernetes, Terraform, MLflow, Weights & Biases, FAISS, Pinecone, Qdrant, Chroma, Controllable Generation, Efficient Inference, Model Alignment, Representation Learning\n",
      "Metadata: {'category': 'skills'}\n",
      "\n",
      "Content: Role: Senior Generative AI Engineer at NeuraLabs Inc. (June 2023 - Present). Responsibilities: Spearheaded end-to-end development of an enterprise LLM platform supporting 10+ internal products; fine-tuned Llama-3-70B and Mistral-7B using QLoRA and DPO, achieving 92% human preference alignment on domain-specific QA tasks. Designed and deployed a multimodal RAG architecture combining CLIP, BLIP-2, and FAISS to ground generative responses in proprietary documentation, reducing hallucination rates by 58%. Led a team of 6 engineers to build a model evaluation framework for generative quality, safety, and latency‚Äîadopted org-wide and integrated into CI/CD pipelines. Reduced cloud inference costs by 65% via dynamic batching, model quantization (GGUF), and speculative decoding; saved $1.2M/year in AWS spend. Authored internal technical standards for prompt versioning, eval harnesses, and ethical AI guardrails.\n",
      "Metadata: {'category': 'work_experience', 'company': 'NeuraLabs Inc.', 'role': 'Senior Generative AI Engineer'}\n",
      "\n",
      "Content: Role: AI Research Intern at Google DeepMind (May 2022 - August 2022). Responsibilities: Researched scalable methods for instruction tuning of vision-language models; contributed to early prototypes of PaLI-X extensions. Developed a contrastive learning objective for aligning text prompts with image semantics, improving zero-shot retrieval accuracy by 11%. Co-authored internal white paper on safety mitigations for generative vision systems.\n",
      "Metadata: {'category': 'work_experience', 'company': 'Google DeepMind', 'role': 'AI Research Intern'}\n",
      "\n",
      "Content: Degree: M.S. in Machine Learning from Stanford University (Graduated: 2019).\n",
      "Metadata: {'category': 'education', 'institution': 'Stanford University'}\n",
      "\n",
      "Content: Degree: Ph.D. in Artificial Intelligence from Stanford University (Graduated: 2023).\n",
      "Metadata: {'category': 'education', 'institution': 'Stanford University'}\n",
      "\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "loaded_db = FAISS.load_local(\"faiss_resume_index\", embeddings, allow_dangerous_deserialization=True)\n",
    "retriever = loaded_db.as_retriever(search_kwargs={\"k\": 53})\n",
    "\n",
    "\n",
    "query = \"\"\n",
    "results = retriever.invoke(query)\n",
    "\n",
    "print(f\"Query: '{query}'\")\n",
    "print(\"--- Search Results ---\")\n",
    "for doc in results:\n",
    "    print(f\"Content: {doc.page_content}\")\n",
    "    print(f\"Metadata: {doc.metadata}\\n\")\n",
    "print(\"----------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4cc0f4b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'John Doe\\nSan Francisco, CA | johndoe@email.com | (123) 456-7890 | linkedin.com/in/johndoe | github.com/johndoe\\n\\nGenerative AI Engineer\\nInnovative and detail-oriented Generative AI Engineer with 1.5 years of experience designing, training, and deploying large language models (LLMs) and multimodal AI systems. Skilled in prompt engineering, fine-tuning transformer architectures, and building scalable generative pipelines using modern ML frameworks.\\n\\nPROFESSIONAL EXPERIENCE\\n\\nAI Engineer\\nNeuraLabs Inc., San Francisco, CA\\nJune 2023 √¢‚Ç¨‚Äú Present\\n\\nDeveloped and deployed a fine-tuned Llama-2-based customer support chatbot, reducing human agent workload by 35% and improving response accuracy by 22%.\\nBuilt a multimodal content generation pipeline using CLIP and Stable Diffusion to auto-generate marketing visuals from text prompts, adopted by 3 internal product teams.\\nOptimized inference latency by 40% through quantization and ONNX runtime integration for generative models in production.\\nCollaborated with product and data teams to implement human-in-the-loop feedback systems for continuous model improvement.\\nPROJECTS\\n\\n1. Personalized Story Generator\\n\\nCreated a fine-tuned T5 model that generates short fiction stories tailored to user preferences (genre, tone, characters).\\nIntegrated retrieval-augmented generation (RAG) to maintain narrative consistency and fact grounding.\\nDeployed as a Flask API with React frontend; achieved 4.7/5 user satisfaction in beta testing.\\n\\n2. CodeWhisperer Clone (Open-Source)\\n\\nTrained a 160M-parameter code generation model on GitHub public repositories using CodeLlama architecture.\\nImplemented context-aware autocompletion for Python and JavaScript with 82% syntactic correctness in evaluations.\\nOpen-sourced on GitHub; received 450+ stars and 30+ forks within 2 months.\\n\\n3. AI-Powered Resume Builder\\n\\nEngineered a generative system that transforms user inputs into ATS-optimized resumes using few-shot prompting with GPT-3.5 Turbo.\\nAdded PDF export and keyword optimization features based on real job descriptions.\\nHosted on AWS Lambda with S3 storage; used by 1,200+ beta users.\\n\\nCERTIFICATIONS\\n\\nAWS Certified Machine Learning √¢‚Ç¨‚Äú Specialty (2024)\\nDeepLearning.AI Generative AI with LLMs Specialization (2023)\\nGoogle Cloud Professional Machine Learning Engineer (2023)\\n\\nTECHNICAL SKILLS\\n\\nLanguages: Python, JavaScript, SQL\\nML Frameworks: PyTorch, Hugging Face Transformers, LangChain, LlamaIndex\\nCloud/DevOps: AWS (SageMaker, Lambda, S3), Docker, FastAPI, Git\\nGenerative AI: LLM fine-tuning (LoRA, QLoRA), RAG, Prompt Engineering, Stable Diffusion, Vector Databases (Pinecone, FAISS)\\n\\nEDUCATION\\n\\nB.S. in Computer Science\\nUniversity of California, Berkeley √¢‚Ç¨‚Äú 2022'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resume_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9865737",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joshua.david\\AppData\\Local\\Temp\\ipykernel_15096\\4070553310.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
      "c:\\Users\\joshua.david\\Downloads\\InterviewBot\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "de5fdfbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Key 'parameters' is not supported in schema, ignoring\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Job Description Processing Pipeline ---\n",
      "Invoking Gemini for agentic extraction of Job Description...\n",
      "Extraction complete.\n",
      "Created 3 semantic chunks for the job description.\n",
      "\n",
      "--- Sample Chunk ---\n",
      "page_content='Responsibilities: Design and develop algorithms for generative models using deep learning techniques. Collaborate with cross-functional teams to integrate generative AI solutions into existing workflow systems. Research and stay up-to-date on the latest advancements in generative AI technologies and methodologies. Optimize and fine-tune generative models for performance and efficiency. Troubleshoot and resolve issues related to generative AI models and implementations. Create and maintain documentation for generative AI models and their applications. Communicate complex technical concepts and findings to non-technical stakeholders.' metadata={'category': 'responsibilities', 'company': 'null', 'job_title': 'Generative AI Engineer'}\n",
      "--------------------\n",
      "\n",
      "Initializing embedding model and FAISS vector store...\n",
      "FAISS index saved locally to 'faiss_jd_index'.\n",
      "--- Pipeline Completed Successfully ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import List, Dict\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LangChain components\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# --- 1. JSON SCHEMA DEFINITION FOR JOB DESCRIPTION ---\n",
    "# Define the desired structured output for a job description.\n",
    "\n",
    "JSON_JD_SCHEMA = {\n",
    "    \"title\": \"JobDescription\",\n",
    "    \"description\": \"The structured representation of a job description.\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"job_title\": {\"type\": \"string\", \"description\": \"The title of the job position.\"},\n",
    "        \"company\": {\"type\": \"string\", \"description\": \"The name of the company hiring.\"},\n",
    "        \"location\": {\"type\": \"string\", \"description\": \"The location of the job (e.g., city, state, remote).\"},\n",
    "        \"company_summary\": {\"type\": \"string\", \"description\": \"A brief summary of the company.\"},\n",
    "        \"responsibilities\": {\n",
    "            \"type\": \"array\",\n",
    "            \"description\": \"A list of key responsibilities for the role.\",\n",
    "            \"items\": {\"type\": \"string\"},\n",
    "        },\n",
    "        \"required_qualifications\": {\n",
    "            \"type\": \"array\",\n",
    "            \"description\": \"A list of essential qualifications and skills.\",\n",
    "            \"items\": {\"type\": \"string\"},\n",
    "        },\n",
    "        \"preferred_qualifications\": {\n",
    "            \"type\": \"array\",\n",
    "            \"description\": \"A list of desired but not essential qualifications.\",\n",
    "            \"items\": {\"type\": \"string\"},\n",
    "        },\n",
    "    },\n",
    "    \"required\": [\"job_title\", \"company\", \"responsibilities\", \"required_qualifications\"],\n",
    "}\n",
    "\n",
    "# --- 2. AGENTIC EXTRACTION (LANGCHAIN + GEMINI) ---\n",
    "\n",
    "def extract_jd_data(jd_text: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Uses LangChain and Gemini to extract structured data from job description text\n",
    "    based on a JSON schema dictionary.\n",
    "    \"\"\"\n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0.2)\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", \"You are an expert recruiting assistant specializing in parsing job descriptions. \"\n",
    "             \"Your task is to extract relevant information from the following job description text and format it as a valid JSON object. \"\n",
    "             \"Adhere strictly to the provided schema. If a piece of information is not found, use null or an empty list.\"),\n",
    "            (\"human\", \"{jd_text}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Chain the prompt and model with the structured output schema\n",
    "    structured_runnable = prompt | llm.with_structured_output(schema=JSON_JD_SCHEMA)\n",
    "\n",
    "    print(\"Invoking Gemini for agentic extraction of Job Description...\")\n",
    "    result = structured_runnable.invoke({\"jd_text\": jd_text})\n",
    "    print(\"Extraction complete.\")\n",
    "    return result\n",
    "\n",
    "# --- 3. SEMANTIC CHUNKING ---\n",
    "\n",
    "def create_jd_semantic_chunks(jd_data: Dict) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Converts the structured JD dictionary into a list of semantic LangChain Documents.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    company = jd_data.get('company', 'N/A')\n",
    "    job_title = jd_data.get('job_title', 'N/A')\n",
    "\n",
    "    # Job Overview chunk\n",
    "    overview_content = (\n",
    "        f\"Job Title: {job_title} at {company}. \"\n",
    "        f\"Location: {jd_data.get('location', 'N/A')}. \"\n",
    "        f\"Company Summary: {jd_data.get('company_summary', '')}\"\n",
    "    )\n",
    "    chunks.append(Document(\n",
    "        page_content=overview_content.strip(),\n",
    "        metadata={\"category\": \"overview\", \"company\": company, \"job_title\": job_title}\n",
    "    ))\n",
    "\n",
    "    # Responsibilities chunk\n",
    "    responsibilities = jd_data.get('responsibilities', [])\n",
    "    if responsibilities:\n",
    "        chunks.append(Document(\n",
    "            page_content=f\"Responsibilities: {' '.join(responsibilities)}\",\n",
    "            metadata={\"category\": \"responsibilities\", \"company\": company, \"job_title\": job_title}\n",
    "        ))\n",
    "\n",
    "    # Required Qualifications chunk\n",
    "    required_qualifications = jd_data.get('required_qualifications', [])\n",
    "    if required_qualifications:\n",
    "        chunks.append(Document(\n",
    "            page_content=f\"Required Qualifications: {' '.join(required_qualifications)}\",\n",
    "            metadata={\"category\": \"required_qualifications\", \"company\": company, \"job_title\": job_title}\n",
    "        ))\n",
    "\n",
    "    # Preferred Qualifications chunk\n",
    "    preferred_qualifications = jd_data.get('preferred_qualifications', [])\n",
    "    if preferred_qualifications:\n",
    "        chunks.append(Document(\n",
    "            page_content=f\"Preferred Qualifications: {' '.join(preferred_qualifications)}\",\n",
    "            metadata={\"category\": \"preferred_qualifications\", \"company\": company, \"job_title\": job_title}\n",
    "        ))\n",
    "\n",
    "    print(f\"Created {len(chunks)} semantic chunks for the job description.\")\n",
    "    return chunks\n",
    "\n",
    "# --- MAIN EXECUTION ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    load_dotenv()\n",
    "\n",
    "    if not os.getenv(\"GOOGLE_API_KEY\"):\n",
    "        raise ValueError(\"GOOGLE_API_KEY not found in environment variables.\")\n",
    "\n",
    "    # --- Build Phase ---\n",
    "    print(\"--- Starting Job Description Processing Pipeline ---\")\n",
    "\n",
    "    with open(\"txts/job_description.txt\", \"r\", encoding='utf-8') as f:\n",
    "        jd_text = f.read()\n",
    "\n",
    "    structured_jd = extract_jd_data(jd_text)\n",
    "    documents = create_jd_semantic_chunks(structured_jd)\n",
    "\n",
    "    print(\"\\n--- Sample Chunk ---\")\n",
    "    if documents:\n",
    "        print(documents[1]) # Print the responsibilities chunk\n",
    "    print(\"--------------------\\n\")\n",
    "\n",
    "    # --- 4. VECTORIZATION & STORAGE ---\n",
    "    print(\"Initializing embedding model and FAISS vector store...\")\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "    db = FAISS.from_documents(documents, embeddings)\n",
    "    db.save_local(\"faiss_jd_index\")\n",
    "    print(\"FAISS index saved locally to 'faiss_jd_index'.\")\n",
    "    print(\"--- Pipeline Completed Successfully ---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89bf241a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'key responsibilities and essential qualifications for this role'\n",
      "--- Search Results ---\n",
      "Content: Required Qualifications: Strong background in machine learning and deep learning algorithms. Proficiency in Python, TensorFlow, and PyTorch for developing AI models. Experience in generative AI techniques such as GANs and VAEs. Ability to design and implement scalable and efficient AI systems. Advanced knowledge of natural language processing for text generation tasks. Familiarity with computer vision and image generation using AI. Skills in data preprocessing and feature engineering for AI model training. Strong understanding of neural network architectures and optimization techniques. Experience in deploying AI models into production environments. Ability to stay updated with the latest advancements in generative AI research and incorporate them into work.\n",
      "Metadata: {'category': 'required_qualifications', 'company': 'null', 'job_title': 'Generative AI Engineer'}\n",
      "\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "loaded_db = FAISS.load_local(\"faiss/faiss_jd_index\", embeddings, allow_dangerous_deserialization=True)\n",
    "retriever = loaded_db.as_retriever(search_kwargs={\"k\": 1})\n",
    "\n",
    "\n",
    "query = \"key responsibilities and essential qualifications for this role\"\n",
    "results = retriever.invoke(query)\n",
    "\n",
    "print(f\"Query: '{query}'\")\n",
    "print(\"--- Search Results ---\")\n",
    "for doc in results:\n",
    "    print(f\"Content: {doc.page_content}\")\n",
    "    print(f\"Metadata: {doc.metadata}\\n\")\n",
    "print(\"----------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48badb0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.10.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
